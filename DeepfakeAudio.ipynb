{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMBs0vcoipE0m05L6vsSotj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4e249b3c47140b69deb755eae5debd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_18bc0539cdb3452b8bee823028077ccf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b646bcdaae6c405e8b0c97921bafa6c4",
              "IPY_MODEL_42e541741b124800a7fcffec381cde3f"
            ]
          }
        },
        "18bc0539cdb3452b8bee823028077ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b646bcdaae6c405e8b0c97921bafa6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_124986c7d7684551a85601ffab8f8bf6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 111898327,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 111898327,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6cd21ce3340476681cca2c52e2de8e8"
          }
        },
        "42e541741b124800a7fcffec381cde3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1da71f8749545fab0f5b490020838e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 107M/107M [03:31&lt;00:00, 530kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10cab0d1f78c4e53a51c27fe865f74c7"
          }
        },
        "124986c7d7684551a85601ffab8f8bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6cd21ce3340476681cca2c52e2de8e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1da71f8749545fab0f5b490020838e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10cab0d1f78c4e53a51c27fe865f74c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepsree24/gemsushant/blob/master/DeepfakeAudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLWRZcReMBdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e7d34b-9187-4394-9c10-e251bb882851"
      },
      "source": [
        "!pip install facenet-pytorch\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting facenet-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e8/5ea742737665ba9396a8a2be3d2e2b49a13804b56a7e7bb101e8731ade8f/facenet_pytorch-2.5.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet-pytorch) (3.7.4.3)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M2_IJ-tk9vo"
      },
      "source": [
        "from facenet_pytorch.models.inception_resnet_v1 import get_torch_home\n",
        "torch_home = get_torch_home()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOlLZXQslCOA",
        "outputId": "3fe6ff19-9233-4217-cc29-c420efaaf248"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# See github.com/timesler/facenet-pytorch:\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running on device: {device}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d4e249b3c47140b69deb755eae5debd2",
            "18bc0539cdb3452b8bee823028077ccf",
            "b646bcdaae6c405e8b0c97921bafa6c4",
            "42e541741b124800a7fcffec381cde3f",
            "124986c7d7684551a85601ffab8f8bf6",
            "a6cd21ce3340476681cca2c52e2de8e8",
            "b1da71f8749545fab0f5b490020838e4",
            "10cab0d1f78c4e53a51c27fe865f74c7"
          ]
        },
        "id": "cO8nkQ7elGEl",
        "outputId": "9812e834-6f85-44fb-fe78-d6c92b4bf7ab"
      },
      "source": [
        "mtcnn = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()\n",
        "\n",
        "# Load facial recognition model\n",
        "resnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4e249b3c47140b69deb755eae5debd2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=111898327.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fjkWbMzlKl9"
      },
      "source": [
        "class DetectionPipeline:\n",
        "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
        "    \n",
        "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n",
        "        \"\"\"Constructor for DetectionPipeline class.\n",
        "        \n",
        "        Keyword Arguments:\n",
        "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
        "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
        "                (default: {None})\n",
        "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
        "            resize {float} -- Fraction by which to resize frames from original prior to facedetection. A value less than 1 results in downsampling and a value greater than\n",
        "                1 result in upsampling. (default: {None})\n",
        "        \"\"\"\n",
        "        self.detector = detector\n",
        "        self.n_frames = n_frames\n",
        "        self.batch_size = batch_size\n",
        "        self.resize = resize\n",
        "        \n",
        "            \n",
        "    def __call__(self, filename):\n",
        "        \"\"\"Load frames from an MP4 video and detect faces.\n",
        "\n",
        "        Arguments:\n",
        "            filename {str} -- Path to video.\n",
        "        \"\"\"\n",
        "        # Create video reader and find length\n",
        "        v_cap = cv2.VideoCapture(filename)\n",
        "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        print(v_len)\n",
        "        # Pick 'n_frames' evenly spaced frames to sample\n",
        "        if self.n_frames is None:\n",
        "            sample = np.arange(0, v_len)\n",
        "        else:\n",
        "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
        "\n",
        "        # Loop through frames\n",
        "        faces = []\n",
        "        frames = []\n",
        "        for j in range(v_len):\n",
        "            success = v_cap.grab()\n",
        "            if j in sample:\n",
        "                # Load frame\n",
        "                success, frame = v_cap.retrieve()\n",
        "                if not success:\n",
        "                    continue\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = Image.fromarray(frame)\n",
        "                \n",
        "                # Resize frame to desired size\n",
        "                if self.resize is not None:\n",
        "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
        "                frames.append(frame)\n",
        "\n",
        "                # When batch is full, detect faces and reset frame list\n",
        "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
        "                    faces.extend(self.detector(frames))\n",
        "                    frames = []\n",
        "\n",
        "        v_cap.release()\n",
        "        \n",
        "        return faces  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t4yYYJIlhnH"
      },
      "source": [
        "import copy\n",
        "def process_faces(faces, resnet):\n",
        "    # Filter out frames without faces\n",
        "    faces = [f for f in faces if f is not None]\n",
        "    if(len(faces) == 0):\n",
        "        return []\n",
        "#     faces = np.array(faces)\n",
        "#     print(faces[200].shape)\n",
        "#     faces = torch.from_numpy(faces)\n",
        "#     f = copy.deepcopy(faces)\n",
        "#     f = np.array(f[0])\n",
        "#     print(type(faces))\n",
        "#     print(f.shape)\n",
        "    faces = torch.cat(faces).to(device)\n",
        "    if(len(faces)<290):\n",
        "        return []\n",
        "#     print(len(faces))\n",
        "    faces = faces[:290]\n",
        "    # Generate facial feature vectors using a pretrained model\n",
        "    embeddings = resnet(faces)\n",
        "    #     print(len(embeddings))\n",
        "#     print(len(embeddings[0]))\n",
        "    \n",
        "#     print(len(embeddings))\n",
        "#     print(len(embeddings[0]))\n",
        "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
        "    centroid = embeddings.mean(dim=0)   \n",
        "    \n",
        "    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n",
        "    return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlDgjFqMlv4e"
      },
      "source": [
        "# Define face detection pipeline\n",
        "detection_pipeline = DetectionPipeline(detector=mtcnn, batch_size=60, resize=0.25)\n",
        "import json\n",
        "\n",
        "with open('../input/deepfake-detection-challenge/train_sample_videos/metadata.json') as f:\n",
        "  data = json.load(f)\n",
        "# Get all test videos\n",
        "filenames = glob.glob('/kaggle/input/deepfake-detection-challenge/train_sample_videos/*.mp4')\n",
        "total_files = len(filenames)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "start = time.time()\n",
        "n_processed = 0\n",
        "# filename = 'kaggle/input/deepfake-detection-challenge/train_sample_videos/aapnvogymq.mp4'\n",
        "# print(filename)\n",
        "# faces = detection_pipeline(filename)\n",
        "# print(faces)\n",
        "with torch.no_grad():\n",
        "    for i, filename in tqdm(enumerate(filenames), total=len(filenames)):\n",
        "        print(i, filename)\n",
        "        try:\n",
        "            # Load frames and find faces\n",
        "            faces = detection_pipeline(filename)\n",
        "           \n",
        "            \n",
        "#             f = np.array(faces[0])\n",
        "#             print(f.shape)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # Calculate embeddings\n",
        "            \n",
        "            z = process_faces(faces, resnet)\n",
        "            if (len(z)!=0):\n",
        "                X.append(z)\n",
        "                if(data[filename[63:]]['label']=='FAKE'):\n",
        "                    y.append(1)\n",
        "                else:\n",
        "                    y.append(0)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\nStopped.')\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            X.append(None)\n",
        "        \n",
        "        n_processed += len(faces)\n",
        "#         print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa1NW9aVmHPW"
      },
      "source": [
        "X=X[1:]\n",
        "y=y[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKwXI3ekmPXg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.85, random_state=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le6_bQVymP7R"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "y_pred_lr = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2UET3MomSpi"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,y_pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}